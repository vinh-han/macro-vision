{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fce7046",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-06T05:03:53.809738Z",
     "iopub.status.busy": "2025-11-06T05:03:53.809232Z",
     "iopub.status.idle": "2025-11-06T05:03:56.688927Z",
     "shell.execute_reply": "2025-11-06T05:03:56.688351Z"
    },
    "papermill": {
     "duration": 2.884676,
     "end_time": "2025-11-06T05:03:56.690276",
     "exception": false,
     "start_time": "2025-11-06T05:03:53.805600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a845aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:03:56.695965Z",
     "iopub.status.busy": "2025-11-06T05:03:56.695683Z",
     "iopub.status.idle": "2025-11-06T05:05:17.236467Z",
     "shell.execute_reply": "2025-11-06T05:05:17.235404Z"
    },
    "papermill": {
     "duration": 80.545097,
     "end_time": "2025-11-06T05:05:17.237977",
     "exception": false,
     "start_time": "2025-11-06T05:03:56.692880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\r\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q pycocotools\n",
    "!pip install -q opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3f12e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:17.275447Z",
     "iopub.status.busy": "2025-11-06T05:05:17.275166Z",
     "iopub.status.idle": "2025-11-06T05:05:27.691971Z",
     "shell.execute_reply": "2025-11-06T05:05:27.691112Z"
    },
    "papermill": {
     "duration": 10.437344,
     "end_time": "2025-11-06T05:05:27.693437",
     "exception": false,
     "start_time": "2025-11-06T05:05:17.256093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Torchvision version: 0.21.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, ssd300_vgg16\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d1da6d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:27.729477Z",
     "iopub.status.busy": "2025-11-06T05:05:27.729050Z",
     "iopub.status.idle": "2025-11-06T05:05:29.059998Z",
     "shell.execute_reply": "2025-11-06T05:05:29.058863Z"
    },
    "papermill": {
     "duration": 1.350159,
     "end_time": "2025-11-06T05:05:29.061394",
     "exception": false,
     "start_time": "2025-11-06T05:05:27.711235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images path: coco128/images/train2017\n",
      "Labels path: coco128/labels/train2017\n",
      "Number of images: 128\n"
     ]
    }
   ],
   "source": [
    "# Download COCO128 dataset\n",
    "!wget -q https://ultralytics.com/assets/coco128.zip\n",
    "!unzip -q coco128.zip\n",
    "!rm coco128.zip\n",
    "\n",
    "# Set paths\n",
    "COCO128_PATH = Path('coco128')\n",
    "IMAGES_PATH = COCO128_PATH / 'images' / 'train2017'\n",
    "LABELS_PATH = COCO128_PATH / 'labels' / 'train2017'\n",
    "\n",
    "print(f\"Images path: {IMAGES_PATH}\")\n",
    "print(f\"Labels path: {LABELS_PATH}\")\n",
    "print(f\"Number of images: {len(list(IMAGES_PATH.glob('*.jpg')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462a0c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:29.098258Z",
     "iopub.status.busy": "2025-11-06T05:05:29.097497Z",
     "iopub.status.idle": "2025-11-06T05:05:29.357572Z",
     "shell.execute_reply": "2025-11-06T05:05:29.356705Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.279873,
     "end_time": "2025-11-06T05:05:29.358838",
     "exception": false,
     "start_time": "2025-11-06T05:05:29.078965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created COCO annotations: coco128/annotations.json\n"
     ]
    }
   ],
   "source": [
    "# Create COCO format annotations for evaluation\n",
    "def create_coco_annotations():\n",
    "    \"\"\"Convert YOLO format to COCO format for evaluation\"\"\"\n",
    "    images = []\n",
    "    annotations = []\n",
    "    ann_id = 1\n",
    "    \n",
    "    for idx, img_path in enumerate(sorted(IMAGES_PATH.glob('*.jpg')), 1):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        images.append({\n",
    "            'id': idx,\n",
    "            'file_name': img_path.name,\n",
    "            'height': h,\n",
    "            'width': w\n",
    "        })\n",
    "        \n",
    "        # Read YOLO format labels\n",
    "        label_path = LABELS_PATH / f\"{img_path.stem}.txt\"\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    \n",
    "                    # Convert YOLO format to COCO format (x, y, width, height)\n",
    "                    x = (x_center - width / 2) * w\n",
    "                    y = (y_center - height / 2) * h\n",
    "                    box_w = width * w\n",
    "                    box_h = height * h\n",
    "                    \n",
    "                    annotations.append({\n",
    "                        'id': ann_id,\n",
    "                        'image_id': idx,\n",
    "                        'category_id': int(class_id) + 1,  # COCO categories start from 1\n",
    "                        'bbox': [x, y, box_w, box_h],\n",
    "                        'area': box_w * box_h,\n",
    "                        'iscrowd': 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "    \n",
    "    # COCO categories (80 classes)\n",
    "    categories = [{'id': i, 'name': f'class_{i}'} for i in range(1, 81)]\n",
    "    \n",
    "    # Complete COCO format with required fields\n",
    "    coco_format = {\n",
    "        'info': {\n",
    "            'description': 'COCO128 Dataset',\n",
    "            'url': 'https://github.com/ultralytics/coco128',\n",
    "            'version': '1.0',\n",
    "            'year': 2024,\n",
    "            'contributor': 'Ultralytics',\n",
    "            'date_created': '2024/01/01'\n",
    "        },\n",
    "        'licenses': [{\n",
    "            'id': 1,\n",
    "            'name': 'Attribution-NonCommercial-ShareAlike License',\n",
    "            'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/'\n",
    "        }],\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': categories\n",
    "    }\n",
    "    \n",
    "    # Save annotations\n",
    "    ann_file = COCO128_PATH / 'annotations.json'\n",
    "    with open(ann_file, 'w') as f:\n",
    "        json.dump(coco_format, f)\n",
    "    \n",
    "    return str(ann_file)\n",
    "\n",
    "annotations_file = create_coco_annotations()\n",
    "print(f\"Created COCO annotations: {annotations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cd738f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:29.394239Z",
     "iopub.status.busy": "2025-11-06T05:05:29.394033Z",
     "iopub.status.idle": "2025-11-06T05:05:36.306025Z",
     "shell.execute_reply": "2025-11-06T05:05:36.305033Z"
    },
    "papermill": {
     "duration": 6.931193,
     "end_time": "2025-11-06T05:05:36.307447",
     "exception": false,
     "start_time": "2025-11-06T05:05:29.376254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading YOLOv11 models...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% ━━━━━━━━━━━━ 38.8MB 143.7MB/s 0.3s\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt to 'yolo11l.pt': 100% ━━━━━━━━━━━━ 49.0MB 192.6MB/s 0.3s\n",
      "Loading Faster R-CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:01<00:00, 166MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SSD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n",
      "100%|██████████| 136M/136M [00:00<00:00, 212MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load YOLOv11 models\n",
    "print(\"\\nLoading YOLOv11 models...\")\n",
    "yolov11m = YOLO('yolo11m.pt')\n",
    "yolov11l = YOLO('yolo11l.pt')\n",
    "\n",
    "# Load Faster R-CNN\n",
    "print(\"Loading Faster R-CNN...\")\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.to(device)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "# Load SSD\n",
    "print(\"Loading SSD...\")\n",
    "ssd = ssd300_vgg16(pretrained=True)\n",
    "ssd.to(device)\n",
    "ssd.eval()\n",
    "\n",
    "print(\"\\nAll models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f715b6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:36.346275Z",
     "iopub.status.busy": "2025-11-06T05:05:36.345679Z",
     "iopub.status.idle": "2025-11-06T05:05:36.352837Z",
     "shell.execute_reply": "2025-11-06T05:05:36.352112Z"
    },
    "papermill": {
     "duration": 0.02754,
     "end_time": "2025-11-06T05:05:36.353977",
     "exception": false,
     "start_time": "2025-11-06T05:05:36.326437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def warm_up_model(model, model_type, num_iterations=10):\n",
    "    \"\"\"Warm up model for accurate timing\"\"\"\n",
    "    dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "    \n",
    "    print(f\"Warming up {model_type}...\")\n",
    "    for _ in range(num_iterations):\n",
    "        if model_type.startswith('yolo'):\n",
    "            model.predict(dummy_input, verbose=False)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                model(dummy_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def measure_inference_time(model, model_type, image_paths, num_runs=3):\n",
    "    \"\"\"Measure inference time and FPS\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            \n",
    "            if model_type.startswith('yolo'):\n",
    "                results = model.predict(img, verbose=False, conf=0.25)\n",
    "            else:\n",
    "                img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(img_tensor)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    avg_time_per_image = avg_time / len(image_paths)\n",
    "    fps = 1.0 / avg_time_per_image\n",
    "    \n",
    "    return avg_time_per_image, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e057aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:36.391649Z",
     "iopub.status.busy": "2025-11-06T05:05:36.391445Z",
     "iopub.status.idle": "2025-11-06T05:05:36.401895Z",
     "shell.execute_reply": "2025-11-06T05:05:36.401379Z"
    },
    "papermill": {
     "duration": 0.030721,
     "end_time": "2025-11-06T05:05:36.402883",
     "exception": false,
     "start_time": "2025-11-06T05:05:36.372162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_yolo_to_coco_results(results, img_id):\n",
    "    \"\"\"Convert YOLO results to COCO format\"\"\"\n",
    "    coco_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0])\n",
    "            cls = int(box.cls[0])\n",
    "            \n",
    "            coco_results.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cls + 1,\n",
    "                'bbox': [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                'score': conf\n",
    "            })\n",
    "    \n",
    "    return coco_results\n",
    "\n",
    "def convert_torchvision_to_coco_results(predictions, img_id):\n",
    "    \"\"\"Convert torchvision model results to COCO format\"\"\"\n",
    "    coco_results = []\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    \n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        coco_results.append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': int(label),\n",
    "            'bbox': [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return coco_results\n",
    "\n",
    "def calculate_map50(model, model_type, annotations_file):\n",
    "    \"\"\"Calculate mAP@0.5 for a model\"\"\"\n",
    "    print(f\"\\nCalculating mAP@0.5 for {model_type}...\")\n",
    "    \n",
    "    coco_gt = COCO(annotations_file)\n",
    "    image_ids = sorted(coco_gt.getImgIds())\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for img_id in image_ids:\n",
    "        img_info = coco_gt.loadImgs(img_id)[0]\n",
    "        img_path = IMAGES_PATH / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        \n",
    "        if model_type.startswith('yolo'):\n",
    "            results = model.predict(img, verbose=False, conf=0.001)\n",
    "            coco_results = convert_yolo_to_coco_results(results, img_id)\n",
    "        else:\n",
    "            img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = model(img_tensor)\n",
    "            coco_results = convert_torchvision_to_coco_results(predictions, img_id)\n",
    "        \n",
    "        all_results.extend(coco_results)\n",
    "    \n",
    "    if not all_results:\n",
    "        return 0.0\n",
    "    \n",
    "    # Save results\n",
    "    results_file = f'results_{model_type}.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(all_results, f)\n",
    "    \n",
    "    # Evaluate\n",
    "    coco_dt = coco_gt.loadRes(results_file)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.params.iouThrs = [0.5]  # mAP@0.5\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    map50 = coco_eval.stats[0]  # AP at IoU=0.5\n",
    "    \n",
    "    return map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684ef1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:05:36.440817Z",
     "iopub.status.busy": "2025-11-06T05:05:36.440621Z",
     "iopub.status.idle": "2025-11-06T05:07:26.229334Z",
     "shell.execute_reply": "2025-11-06T05:07:26.228360Z"
    },
    "papermill": {
     "duration": 109.809295,
     "end_time": "2025-11-06T05:07:26.230858",
     "exception": false,
     "start_time": "2025-11-06T05:05:36.421563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on 128 images\n",
      "\n",
      "============================================================\n",
      "Evaluating YOLOv11m\n",
      "============================================================\n",
      "Warming up yolov11m...\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.018654823303223. Dividing input by 255.\n",
      "\n",
      "Measuring inference time...\n",
      "\n",
      "Calculating mAP@0.5 for yolov11m...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.29s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.780\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.807\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.943\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=  1 ] = 0.535\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets= 10 ] = 0.833\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.883\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.689\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.928\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.980\n",
      "\n",
      "Results for YOLOv11m:\n",
      "  mAP@0.5: 78.01%\n",
      "  Inference Time: 21.30 ms\n",
      "  FPS: 46.95\n",
      "\n",
      "============================================================\n",
      "Evaluating YOLOv11l\n",
      "============================================================\n",
      "Warming up yolov11l...\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.922115325927734. Dividing input by 255.\n",
      "\n",
      "Measuring inference time...\n",
      "\n",
      "Calculating mAP@0.5 for yolov11l...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.769\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.474\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.810\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.917\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=  1 ] = 0.527\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets= 10 ] = 0.820\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.865\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.673\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.944\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.955\n",
      "\n",
      "Results for YOLOv11l:\n",
      "  mAP@0.5: 76.86%\n",
      "  Inference Time: 24.55 ms\n",
      "  FPS: 40.73\n",
      "\n",
      "============================================================\n",
      "Evaluating Faster R-CNN\n",
      "============================================================\n",
      "Warming up faster_r_cnn...\n",
      "\n",
      "Measuring inference time...\n",
      "\n",
      "Calculating mAP@0.5 for faster_r_cnn...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.41s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.098\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=  1 ] = 0.094\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets= 10 ] = 0.142\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.194\n",
      "\n",
      "Results for Faster R-CNN:\n",
      "  mAP@0.5: 9.83%\n",
      "  Inference Time: 113.41 ms\n",
      "  FPS: 8.82\n",
      "\n",
      "============================================================\n",
      "Evaluating SSD300\n",
      "============================================================\n",
      "Warming up ssd300...\n",
      "\n",
      "Measuring inference time...\n",
      "\n",
      "Calculating mAP@0.5 for ssd300...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.43s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.13s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.100\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=  1 ] = 0.072\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets= 10 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=100 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=100 ] = 0.136\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=100 ] = 0.168\n",
      "\n",
      "Results for SSD300:\n",
      "  mAP@0.5: 7.92%\n",
      "  Inference Time: 37.83 ms\n",
      "  FPS: 26.44\n"
     ]
    }
   ],
   "source": [
    "# Get list of images for timing\n",
    "image_paths = sorted(list(IMAGES_PATH.glob('*.jpg')))\n",
    "print(f\"\\nEvaluating on {len(image_paths)} images\")\n",
    "\n",
    "# Store results\n",
    "results_data = {\n",
    "    'Model': [],\n",
    "    'mAP@0.5': [],\n",
    "    'Inference Time (ms)': [],\n",
    "    'FPS': []\n",
    "}\n",
    "\n",
    "models_to_evaluate = [\n",
    "    (yolov11m, 'YOLOv11m'),\n",
    "    (yolov11l, 'YOLOv11l'),\n",
    "    (faster_rcnn, 'Faster R-CNN'),\n",
    "    (ssd, 'SSD300')\n",
    "]\n",
    "\n",
    "for model, model_name in models_to_evaluate:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Warm up\n",
    "    warm_up_model(model, model_name.lower().replace(' ', '_').replace('-', '_'))\n",
    "    \n",
    "    # Measure inference time\n",
    "    print(f\"\\nMeasuring inference time...\")\n",
    "    avg_time, fps = measure_inference_time(\n",
    "        model, \n",
    "        model_name.lower().replace(' ', '_').replace('-', '_'),\n",
    "        image_paths\n",
    "    )\n",
    "    \n",
    "    # Calculate mAP@0.5\n",
    "    map50 = calculate_map50(\n",
    "        model,\n",
    "        model_name.lower().replace(' ', '_').replace('-', '_'),\n",
    "        annotations_file\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results_data['Model'].append(model_name)\n",
    "    results_data['mAP@0.5'].append(map50 * 100)  # Convert to percentage\n",
    "    results_data['Inference Time (ms)'].append(avg_time * 1000)  # Convert to ms\n",
    "    results_data['FPS'].append(fps)\n",
    "    \n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  mAP@0.5: {map50*100:.2f}%\")\n",
    "    print(f\"  Inference Time: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"  FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537da5e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:07:26.272159Z",
     "iopub.status.busy": "2025-11-06T05:07:26.271926Z",
     "iopub.status.idle": "2025-11-06T05:07:26.294907Z",
     "shell.execute_reply": "2025-11-06T05:07:26.294044Z"
    },
    "papermill": {
     "duration": 0.04478,
     "end_time": "2025-11-06T05:07:26.296134",
     "exception": false,
     "start_time": "2025-11-06T05:07:26.251354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "       Model   mAP@0.5  Inference Time (ms)       FPS\n",
      "    YOLOv11m 78.012838            21.299501 46.949457\n",
      "    YOLOv11l 76.862296            24.552766 40.728608\n",
      "Faster R-CNN  9.830760           113.412481  8.817372\n",
      "      SSD300  7.920735            37.825399 26.437262\n",
      "\n",
      "Results saved to 'model_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "077d7029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T05:07:26.338859Z",
     "iopub.status.busy": "2025-11-06T05:07:26.338486Z",
     "iopub.status.idle": "2025-11-06T05:07:26.347132Z",
     "shell.execute_reply": "2025-11-06T05:07:26.346436Z"
    },
    "papermill": {
     "duration": 0.030678,
     "end_time": "2025-11-06T05:07:26.348240",
     "exception": false,
     "start_time": "2025-11-06T05:07:26.317562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Best Accuracy (mAP@0.5): YOLOv11m with 78.01%\n",
      "Fastest (FPS): YOLOv11m with 46.95 FPS\n",
      "Lowest Latency: YOLOv11m with 21.30 ms\n",
      "\n",
      "Best Overall Efficiency: YOLOv11m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best model for each metric\n",
    "best_map = df_results.loc[df_results['mAP@0.5'].idxmax()]\n",
    "best_fps = df_results.loc[df_results['FPS'].idxmax()]\n",
    "best_time = df_results.loc[df_results['Inference Time (ms)'].idxmin()]\n",
    "\n",
    "print(f\"\\nBest Accuracy (mAP@0.5): {best_map['Model']} with {best_map['mAP@0.5']:.2f}%\")\n",
    "print(f\"Fastest (FPS): {best_fps['Model']} with {best_fps['FPS']:.2f} FPS\")\n",
    "print(f\"Lowest Latency: {best_time['Model']} with {best_time['Inference Time (ms)']:.2f} ms\")\n",
    "\n",
    "# Calculate efficiency score (mAP / inference_time)\n",
    "df_results['Efficiency Score'] = df_results['mAP@0.5'] / df_results['Inference Time (ms)']\n",
    "best_efficiency = df_results.loc[df_results['Efficiency Score'].idxmax()]\n",
    "print(f\"\\nBest Overall Efficiency: {best_efficiency['Model']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 217.908341,
   "end_time": "2025-11-06T05:07:27.988891",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-06T05:03:50.080550",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
